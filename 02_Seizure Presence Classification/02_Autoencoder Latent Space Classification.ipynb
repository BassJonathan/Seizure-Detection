{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epileptic Seizure Classification with an Autoencoder and Classification of the Latent Space\n",
    "This notebook contains the classification of time series EEG data for the detection of epileptic seizures based on the preprocessed CHB-MIT Scalp EEG Database using an autoencoder- and classification-model.<br>\n",
    "The codes is structured as followed:\n",
    "1. [Imports](#1-imports)\n",
    "2. [Load Preprocessed Dataset](#2-load-preprocessed-dataset)\n",
    "3. [Split Dataset](#3-split-dataset)\n",
    "4. [Normalize Dataset](#4-normalize-dataset)\n",
    "5. [Autoencoder](#5-autoencoder) <br>\n",
    "5.1 [Define Autoencoder-Model](#51-define-autoencoder-model) <br>\n",
    "5.2 [Compile Autoencoder-Model](#52-compile-autoencoder-model) <br>\n",
    "5.3 [Train Autoencoder](#53-fit-autoencoder-model) <br>\n",
    "5.4 [Visualize Reconstruction Error](#54-visualize-reconstruction)\n",
    "6. [Split Autoencoder at Latent Space](#6-seperate-encoder)\n",
    "7. [Binary Classification](#7-binary-classification) <br>\n",
    "7.1 [Define Classification-Model](#71-define-classification-model) <br>\n",
    "7.2 [Compile Classification-Model](#72-compile-classification-model) <br>\n",
    "7.3 [Train Classificator](#73-fit-classification-model) <br>\n",
    "8. [Validate Results](#8-validate-results)\n",
    "9. [Conclusion](#9-conclusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import requiered libraries. <br>\n",
    "External packages can be installed via the `pip install -r requirements.txt` command or the notebook-cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\") # Suppress SHAP warnings\n",
    "\n",
    "# Import datascience libraries\n",
    "import numpy as np\n",
    "\n",
    "# Import preprocessing-libraries, classification metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "# Import visualization libraries\n",
    "import plotly.graph_objects as go\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Import neural network framework & layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Import explainability library\n",
    "import shap\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Dataset\n",
    "In order to load the preprocessed dataset, that was created with the notebook `00_Preprocessing.ipynb`, is loaded and the numpy Arrays for the features and labels are extracted. <br>\n",
    "To enshure a functional distribution of the classes in the dataset, the classes with the respective amounts are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('../00_Data/Processed-Data/classification_dataset_max.npz') # Load compressed numpy array\n",
    "X = dataset[\"features\"] # Extract feature-array from compressed file\n",
    "y = dataset[\"labels\"] # Extract label-array from compressed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes: \\n X:\", X.shape, \"y:\", y.shape)\n",
    "print(\"Unique Values:\", np.unique(y, return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Dataset\n",
    "In order to validate and test the trained classifier, the dataset must be split into a `train`, `test`, and `validation` subset. <br>\n",
    "To preserve an equal distribution within each split, the `stratify`-option is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.4, shuffle=True, stratify=np.ravel(y), random_state=34)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5, shuffle=True, stratify=np.ravel(y_rest), random_state=34)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize Dataset\n",
    "When working with neural networks, it is imperative to normalize the data bevore training and testing. This enshures a faster training, avoids numerical instablities and provides a better generalization of the neural network. However with EEG-data, there are additional requirements due to the different characteristics and value-ranges of the individual channels. Therefore, the normalization is done channel by channel based on the training-subset and applied on the test- and validation-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X_train:np.ndarray, X_test:np.ndarray, X_val:np.ndarray, use_standard_scaler:bool=False) -> tuple:\n",
    "    if(use_standard_scaler):\n",
    "        scaler = StandardScaler() # Create Z-Score normalizer\n",
    "    else:\n",
    "        scaler = MinMaxScaler() # Create Min-Max normalizer\n",
    "    X_train_norm = np.zeros(shape=(X_train.shape), dtype='float32') # Create empty array for normalized train-data\n",
    "    X_test_norm = np.zeros(shape=(X_test.shape), dtype='float32') # Create empty array for normalized test-data\n",
    "    X_val_norm = np.zeros(shape=(X_val.shape), dtype='float32') # Create empty array for normalized val-data\n",
    "    for feature_col in range(X_train.shape[2]): # Iterate over features in dataset\n",
    "        X_train_norm[:,:,feature_col] = scaler.fit_transform(X_train[:,:,feature_col]) # Fit and apply normalizer on current feature in train subset\n",
    "        X_test_norm[:,:,feature_col] = scaler.transform(X_test[:,:,feature_col]) # Apply normalizer on current feature in test subset\n",
    "        X_val_norm[:,:,feature_col] = scaler.transform(X_val[:,:,feature_col]) # Apply normalizer on current feature in val subset\n",
    "    return X_train_norm, X_test_norm, X_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized, X_test_normalized, X_val_normalized = normalize_features(X_train, X_test, X_val, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autoencoder\n",
    "The following section contains the data-preperation, build and training of the autoencoder. <br>\n",
    "\n",
    "<b>What is an autoencoder?</b><br>\n",
    "An autoencoder is a neural network architecture, that is used for unsupervised machine learning tasks. It consists out of two main components: The encoder & decoder. The encoder takes the input data and transforms it into a lower dimensional representation of the data, the so-called \"latent-space\". The decoder-part takes this data and tries to reconstruct the original input data. The main target during the training-phase is to minimize the reconstruction error. <br>\n",
    "\n",
    "<b>How can autoencoders be used for the detection of epileptic seizures in EEG-data?</b><br>\n",
    "There are two options how autoencoders can be used for the detection of epileptic seizures in EEG-data: Reconstruction-Error & Latent-Space. <br>\n",
    "By training the autoencoder only on data that does not contain any epileptic seizures, the reconstruction error for \"normal\" data is minimized. That means that if a sample with an active seizure is predicted, the reconstruction error will be increased. By defining an error-threshold, a binary classification can be performed to seperate normal samples from samples with an epileptic seizure.\n",
    "\n",
    "The second option is to use the latent space for the classification. The autoencoder is trained on the complete dataset with the same task of minimizing the reconstruction error. By seperating the decoding-component from the autoencoder, the latent space is exposed. Because of the differences in the data when an epileptic seizure is present, the representation of these samples must be different in the reduced space. Based on this assumption, a classification by using a clustering-approach can be done.\n",
    "\n",
    "The following code contains the second approach.\n",
    "### 5.1 Define Autoencoder-Model\n",
    "**Adam** is used as the optimizer and the **binary crossentropy** used for the loss function.\n",
    "\n",
    "*Note: This project was build on M1 mac. To enshure best performance, the legacy version of the Adam optimizer was used. This can be changed when using windows or newer Versions of Tensorflow!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_ae(train_shape:tuple, initla_lr:float=0.0001):\n",
    "    inputs = Input(shape=(train_shape[1], train_shape[2]))\n",
    "    # Encoder\n",
    "    E1 = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    E2 = MaxPooling1D(pool_size=2, padding='same')(E1)\n",
    "    E3 = Conv1D(filters=12, kernel_size=3, activation='relu', padding='same')(E2)\n",
    "    E4 = MaxPooling1D(pool_size=2, padding='same')(E3)\n",
    "    latent_vector = Conv1D(filters=6, kernel_size=3, activation='relu', padding='same')(E4)\n",
    "    # Decoder\n",
    "    D1 = Conv1D(filters=6, kernel_size=3, activation='relu', padding='same')(latent_vector)\n",
    "    D2 = UpSampling1D(size=2)(D1)\n",
    "    D3 = Conv1D(filters=12, kernel_size=3, activation='relu', padding='same')(D2)\n",
    "    D4 = UpSampling1D(size=2)(D3)\n",
    "    D5 = Conv1D(filters=24, kernel_size=3, activation='relu', padding='same')(D4)\n",
    "    ae_outputs = Dense(train_shape[2])(D5)\n",
    "\n",
    "    autoencoder = Model(inputs=inputs, outputs=ae_outputs)\n",
    "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "    autoencoder.compile(optimizer=opt, loss='mse')\n",
    "    return autoencoder, inputs, latent_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compile Autoencoder-Model\n",
    "In order to train the neural network, the model defined in the previous step must be compiled. Furthermore, two **callbacks** are defined: **Early Stopping** and a **dynamic Learning Rate**. Both callbacks monitor the validation loss and stop the fitting process after a defined number of epochs without any improvement or reduce the learing rate to enable better results. Finally, the model is plottet to visualize the structure, number of parameters and enable the detection of errors in the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, ae_inputs, latent_vector = build_and_compile_ae(X_train.shape, 0.001)\n",
    "\n",
    "earlystopper_ae = EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=10, restore_best_weights=True, verbose=1)\n",
    "reduce_lr_ae = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0000001, verbose=1, cooldown=10)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Fit Autoencoder-Model\n",
    "Fitting the neural network stands for the training process of the weights and bias of each layer to enable good predictions. The normalized training data is used for the training step and a validation is performed after every epoch via the normalized validation split to detect and prevent overfitting. Different to a \"normal\" training process using the features and labels, the feature-data is used for both input and output. After training, the progression of the training and validation loss is visualized for the easy visualization of the fit-process and further detection of overfitting or other issues during the training. In addition, an example of the original and reconstructed data is plottet to see the capabilities of the autoencoder. Finally, the model is saved as an .h5 file to be able to load the model an/or weights during later testing without new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ae = autoencoder.fit(\n",
    "    X_train_normalized, \n",
    "    X_train_normalized,\n",
    "    epochs=500,\n",
    "    batch_size=50,\n",
    "    validation_data=(X_val_normalized, X_val_normalized),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopper_ae, reduce_lr_ae]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data = [\n",
    "        go.Scatter(y=history_ae.history['loss'], name=\"train\"),\n",
    "        go.Scatter(y=history_ae.history['val_loss'], name=\"val\"),\n",
    "    ],\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [MSE]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Reconstruction Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.transpose(X_train_normalized, (0,2,1))\n",
    "data = []\n",
    "for i in a[0]:\n",
    "    data.append(\n",
    "        go.Scatter(y=i)\n",
    "    )\n",
    "\n",
    "fig = go.Figure(\n",
    "    data = data,\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [MSE]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Model Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = autoencoder.predict(X_train_normalized[:1])\n",
    "a = np.transpose(pred, (0,2,1))\n",
    "data = []\n",
    "for i in a[0]:\n",
    "    data.append(\n",
    "        go.Scatter(y=i)\n",
    "    )\n",
    "\n",
    "fig = go.Figure(\n",
    "    data = data,\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [MSE]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Model Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('../99_Assets/01_Saved Models/03_autoencoder_latent_space_clf.h5')\n",
    "# autoencoder = tf.keras.models.load_model('../99_Assets/01_Saved Models/03_autoencoder_latent_space_clf.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Seperate Encoder\n",
    "In order to classify the latent space, the autoencoder-model must be split in half and the latent vector extracted. To prevent overwriting the already trained weights and biases, the `trainable` parameter is set to false. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=ae_inputs, outputs=latent_vector)\n",
    "\n",
    "for layer in encoder.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification\n",
    "After the creation, training and splitting of the autoencoder, the classification of the latent space can be performed. \n",
    "\n",
    "### 7.1 Define Classification-Model\n",
    "Due to the latent space having being two-dimensional (timesteps, convolutional filters), CNNs are also used at the upper layers of the classification model. The information is further reduced with decreasing filters and pooling layers and finally flattend into a one-dimensional representation. This is further processed with dense-layers and finally restriced to one perceptron with an sigmoid activation function to limit the output between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_clf(encoder):\n",
    "    classification_model = tf.keras.Sequential()\n",
    "    classification_model.add(encoder)\n",
    "    classification_model.add(Conv1D(filters=6, kernel_size=3, padding='same'))\n",
    "    classification_model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    classification_model.add(BatchNormalization())\n",
    "    classification_model.add(Conv1D(filters=4, kernel_size=3, padding='same'))\n",
    "    classification_model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    classification_model.add(BatchNormalization())\n",
    "    classification_model.add(Conv1D(filters=2, kernel_size=3, padding='same'))\n",
    "    classification_model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    classification_model.add(BatchNormalization())\n",
    "    classification_model.add(Flatten())\n",
    "    classification_model.add(Dense(32))\n",
    "    classification_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "    classification_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return classification_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Compile Classification-Model\n",
    "In order to train the neural network, the model defined in the previous step must be compiled. Furthermore, two **callbacks** are defined: **Early Stopping** and a **dynamic Learning Rate**. Both callbacks monitor the validation loss and stop the fitting process after a defined number of epochs without any improvement or reduce the learing rate to enable better results. Finally, the model is plottet to visualize the structure, number of parameters and enable the detection of errors in the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = build_and_compile_clf(encoder)\n",
    "\n",
    "earlystopper_clf = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=1)\n",
    "reduce_lr_clf = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0000001, verbose=1, cooldown=10)\n",
    "\n",
    "clf.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Fit Classification-Model\n",
    "Fitting the neural network stands for the training process of the weights and bias of each layer to enable good predictions. The normalized training data is used for the training step and a validation is performed after every epoch via the normalized validation split to detect and prevent overfitting. After training, the progression of the training and validation loss is visualized for the easy visualization of the fit-process and further detection of overfitting or other issues during the training. Finally, the model is saved as an .h5 file to be able to load the model an/or weights during later testing without new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_clf = clf.fit(\n",
    "    X_train_normalized, \n",
    "    y_train, \n",
    "    epochs=1250, \n",
    "    batch_size=50,\n",
    "    validation_data=(X_val_normalized, y_val),\n",
    "    verbose=1, \n",
    "    callbacks=[earlystopper_clf, reduce_lr_clf]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data = [\n",
    "        go.Scatter(y=history_clf.history['loss'], name=\"train\"),\n",
    "        go.Scatter(y=history_clf.history['val_loss'], name=\"val\"),\n",
    "    ],\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [Binary Crossentropy]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Model Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../99_Saved Models/latent_clf.h5')\n",
    "# model = tf.keras.models.load_model('../99_Saved Models/latent_clf.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validate Results\n",
    "To ensure correct training without overfitting and to demonstrate the generalizability of the model, a validation step is performed last. The test subset, which was not seen by the neural network during training, serves as the data basis for this. Therefore, the obtained results can be used as a representation of the generalistic predictive ability of the model. Since, depending on the data set, there may be an inequality in the distribution of the classes, the accuracy is not used as the discriminating metric. \n",
    "\n",
    "The F1 score, G-Mean, the AUC of the ROC both as well as the basic Precision and Recall are calculated in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = clf.predict(X_test_normalized)\n",
    "y_test_predictions = (y_test_predictions >= 0.5).astype(int)\n",
    "f1score = f1_score(y_test, y_test_predictions)\n",
    "gm = geometric_mean_score(y_test, y_test_predictions, average=\"binary\")\n",
    "auc = roc_auc_score(y_test, y_test_predictions, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_test_predictions)\n",
    "recall = recall_score(y_test, y_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"F1-Score\", \"G-Mean\", \"AUC\", \"Precision\", \"Recall\"], [f1score, gm, auc, precision, recall]]\n",
    "table = PrettyTable(data[0])\n",
    "table.add_rows(data[1:])\n",
    "print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
