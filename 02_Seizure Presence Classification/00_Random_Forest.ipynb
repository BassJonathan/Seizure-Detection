{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epileptic Seizure Classification with Random Forest\n",
    "This notebook contains the classification of time series EEG data for the detection of epileptic seizures based on the preprocessed CHB-MIT Scalp EEG Database using a Random Forest classifier. <br>\n",
    "The codes is structured as followed:\n",
    "1. [Imports](#1-imports)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Split Dataset](#3-split-dataset)\n",
    "4. [Define Space & Optimization Function](#4-define-space--optimization-function)\n",
    "5. [Train Optimized Classifier](#5-optimize-classifier)\n",
    "6. [Validate Results](#6-validate-results)\n",
    "7. [Explain Classifier with SHAP](#7-explain-classifier-with-shap)\n",
    "8. [Conclusions](#8-conclusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import requiered libraries. <br>\n",
    "External packages can be installed via the `pip install -r requirements.txt` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in libraries\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")\n",
    "\n",
    "# Import datascience libraries\n",
    "import numpy as np\n",
    "\n",
    "# Import preprocessing-libraries, classifier & metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, make_scorer, classification_report\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "# Import visualization libraries\n",
    "import plotly.graph_objects as go\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Import optimization library\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK, Trials, STATUS_STRINGS\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# Import explainability library\n",
    "import shap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "In order to load the preprocessed dataset, that was created with the notebook `00_Preprocessing.ipynb`, is loaded and the numpy Arrays for the features and labels are extracted. <br>\n",
    "To enshure a functional distribution of the classes in the dataset, the classes with the respective amounts are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('../00_Data/Processed-Data/classification_dataset_mean.npz')\n",
    "X = dataset[\"features\"]\n",
    "y = dataset[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes: \\n X:\", X.shape, \"y:\", y.shape)\n",
    "print(\"Unique Values:\", np.unique(y, return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify the time series with the Random Forest Classifier, the data must be reshaped into two dimensions and the features flattend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_timesteps, n_features = X.shape\n",
    "X_reshaped = np.reshape(X, (n_samples, (n_timesteps * n_features)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Dataset\n",
    "In order to validate and test the trained classifier, the dataset must be split into a `train` and `validation` subset. Due to the applied cross validation, a `test` subset is not needed. <br>\n",
    "To preserve an equal distribution within each split, the `stratify`-option is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y, test_size=0.3, shuffle=True, stratify=np.ravel(y), random_state=34)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Space & Optimization Function\n",
    "To get the best possible predictions, the hyperparameters of the classifier are optimized with the bayesian optimization library `hyperopt`. <br>\n",
    "First, the space for each hyperparameter is defined and stored as an dictionary. <br>\n",
    "The `objective()`-function contains the definition, training and evaluation of the classifier, which is done by a five-fold cross-validation split. <br>\n",
    "Last, the metrics are returned to enable a correct optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_values = ['sqrt','log2', None]\n",
    "\n",
    "space={\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 600, 10)),\n",
    "    'max_depth': hp.quniform('max_depth', 100, 400, 10),\n",
    "    #'min_samples_split' : hp.uniform ('min_samples_split', 0, 0.15),\n",
    "    #'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.25),\n",
    "    'max_features': hp.choice('max_features', max_features_values),\n",
    "}\n",
    "\n",
    "gm_scorer = make_scorer(geometric_mean_score, greater_is_better=True, average='macro') #Create Scorer for G-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    global X_train, y_train, X_test, y_test\n",
    "\n",
    "    # Create classifier\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators = int(space[\"n_estimators\"]),\n",
    "        max_depth = int(space[\"max_depth\"]),\n",
    "        #min_samples_split = space[\"min_samples_split\"],\n",
    "        #min_samples_leaf = space[\"min_samples_leaf\"],\n",
    "        max_features=space[\"max_features\"],\n",
    "        random_state=456,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Cross Validation\n",
    "    splits = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    cross_val = cross_validate(rf_classifier, X_train, np.ravel(y_train), cv=splits, scoring={'f1_macro': 'f1_macro', 'f1_weighted': 'f1_weighted', 'auc': 'roc_auc_ovr', 'gmean': gm_scorer, 'precision': 'precision_macro', 'recall': 'recall_macro', 'waccuracy': 'balanced_accuracy'})\n",
    "    try:\n",
    "        cv_f1_macro = np.mean(cross_val.get('test_f1_macro')[~np.isnan(cross_val.get('test_f1_macro'))])\n",
    "        cv_f1_weighted = np.mean(cross_val.get('test_f1_weighted')[~np.isnan(cross_val.get('test_f1_weighted'))])\n",
    "        cv_auc = np.mean(cross_val.get('test_auc')[~np.isnan(cross_val.get('test_auc'))])\n",
    "        cv_gmean = np.mean(cross_val.get('test_gmean')[~np.isnan(cross_val.get('test_gmean'))])\n",
    "        cv_precision = np.mean(cross_val.get('test_precision')[~np.isnan(cross_val.get('test_precision'))])\n",
    "        cv_recall = np.mean(cross_val.get('test_recall')[~np.isnan(cross_val.get('test_recall'))])\n",
    "        cv_acc_weighted = np.mean(cross_val.get('test_waccuracy')[~np.isnan(cross_val.get('test_waccuracy'))])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\n",
    "            'loss': 1, \n",
    "            'status': STATUS_STRINGS[4], \n",
    "            'metrics': {\n",
    "                'cv_f1_macro': -1,\n",
    "                'cv_f1_weighted': -1,\n",
    "                'cv_auc': -1,\n",
    "                'cv_gmean': -1,\n",
    "                'cv_precision': -1,\n",
    "                'cv_recall': -1,\n",
    "                'cv_acc_weighted': -1\n",
    "            },\n",
    "            'eval_time': time.time()\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'loss': -cv_f1_macro, \n",
    "        'status': STATUS_OK, \n",
    "        'metrics': {\n",
    "            'cv_f1_macro': cv_f1_macro,\n",
    "            'cv_f1_weighted': cv_f1_weighted,\n",
    "            'cv_auc': cv_auc,\n",
    "            'cv_gmean': cv_gmean,\n",
    "            'cv_precision': cv_precision,\n",
    "            'cv_recall': cv_recall,\n",
    "            'cv_acc_weighted': cv_acc_weighted\n",
    "        },\n",
    "        'eval_time': time.time()\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimize Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_param = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Results\n",
    "To ensure correct training without overfitting and to demonstrate the generalizability of the model, a validation step is performed last. The `val` subset, which was not seen by the neural network during training, serves as the data basis for this. Therefore, the obtained results can be used as a representation of the generalistic predictive ability of the random forest model. Since, depending on the data set, there may be an imbalance in the distribution of the classes, the accuracy is not used as the discriminating metric. \n",
    "\n",
    "The `F1-Score`, `G-Mean`, the `AUC of the ROC` both as well as the basic Precision and Recall are calculated in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators = int(best_param['n_estimators']),\n",
    "    max_depth = int(best_param[\"max_depth\"]),\n",
    "    min_samples_split = best_param[\"min_samples_split\"],\n",
    "    min_samples_leaf = best_param[\"min_samples_leaf\"],\n",
    "    max_features = max_features_values[best_param[\"max_features\"]],\n",
    "    random_state = 456,\n",
    "    n_jobs = -1,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit(\n",
    "    X=X_train,\n",
    "    y=np.ravel(y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(rf_classifier, \"./00_random_forest.joblib\")\n",
    "# rf_classifier = joblib.load(\"./00_random_forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = rf_classifier.predict(X_val) #Predict X_test\n",
    "y_val_pred_proba = rf_classifier.predict_proba(X_val) #Predict probablities X_test\n",
    "f1 = f1_score(y_val, y_val_pred, average=\"macro\") #Compute f1-score\n",
    "auc = roc_auc_score(np.ravel(y_val), y_val_pred_proba[:,1], average=\"macro\", multi_class=\"ovr\") #Compute AUC\n",
    "gmean = geometric_mean_score(y_val, y_val_pred, average=\"macro\") #Compute G-Mean\n",
    "precision = precision_score(y_val, y_val_pred)\n",
    "recall = recall_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = PrettyTable([[\"F1-Score\", \"G-Mean\", \"Precision\", \"Recall\"]])\n",
    "table.add_rows([f1, auc, gmean, precision, recall])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explain Classifier with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(rf_classifier)\n",
    "shap_values = explainer.shap_values(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "The goal of this notebook was to demonstrate a binary classification of time series EEG data for epileptic seizure detection by using a Random Forest classifier. To begin, the preprocessed dataset was loaded, the feature dimension was flattened and split into a training and validation subset. In order to build the best possible classification model, hyperparameter optimization was performed and the optimization space was defined first for this purpose. The optimization was performed using the Hyperopt library, which is based on Bayesian mathematics. The objective function includes the definition, training and evaluation of the random forest classifier. In addition, a 5-fold cross validation was performed. The used hyperparameters as well as the results are passed to the minimization function and stored there. After the successful optimization, the hyperparameters with the best result were extracted and an optimized random forest classifier was created. This was also trained and validated with the help of not yet used validation data. Finally, a certain explanatory power of the models was achieved with the help of the SHAP library, which, however, does not provide a target-oriented evaluation of the functioning due to the reduced dimension. \n",
    "\n",
    "In general, the approach using a random forest classifier has proven to be usable. However, the model only achieved an F1 score of XX, which limits its real-world applicability. A final comparison of all methodologies will be made in Notebok XYZ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
