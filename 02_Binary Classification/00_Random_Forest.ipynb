{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epileptic Seizure Classification with Random Forest\n",
    "This notebook contains the classification of time series EEG data for the detection of epileptic seizures based on the preprocessed CHB-MIT Scalp EEG Database.\n",
    "The codes is structured as followed:\n",
    "1. [Imports](#1-imports)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Split Dataset](#3-split-dataset)\n",
    "4. [Define Space & Optimization Function](#4-define-space--optimization-function)\n",
    "5. [Train Optimized Classifier](#5-optimize-classifier)\n",
    "6. [Validate Results](#6-validate-results)\n",
    "7. [Explain Classifier with SHAP](#7-explain-classifier-with-shap)\n",
    "8. [Conclusions](#8-conclusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import requiered libraries. <br>\n",
    "External packages can be installed via the `pip install` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in libraries\n",
    "import time\n",
    "\n",
    "# Import datascience libraries\n",
    "import numpy as np\n",
    "\n",
    "# Import preprocessing-libraries, classifier & metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, make_scorer\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "# Import optimization library\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK, Trials, STATUS_STRINGS\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# Import explainability library\n",
    "import shap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "In order to load the preprocessed dataset, that was created with the notebook `00_Preprocessing.ipynb`, is loaded and the numpy Arrays for the features and labels are extracted. <br>\n",
    "To enshure a functional distribution of the classes in the dataset, the classes with the respective amounts are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('../00_Data/Processed-Data/classification_dataset_majority.npz')\n",
    "X = dataset[\"features\"]\n",
    "y = dataset[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes: \", X.shape, y.shape)\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_timesteps, n_features = X.shape\n",
    "X_reshaped = np.reshape(X, (n_samples, (n_timesteps * n_features)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Dataset\n",
    "In order to validate and test the trained classifier, the dataset must be split into a `train`, `test`, and `validation` subset. <br>\n",
    "To preserve an equal distribution within each split, the `stratify`-option is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y, test_size=0.3, shuffle=True, stratify=np.ravel(y), random_state=34)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Space & Optimization Function\n",
    "To get the best possible predictions, the hyperparameters of the classifier are optimized with the bayesian optimization library `hyperopt`. <br>\n",
    "First, the space for each hyperparameter is defined and stored as an dictionary. <br>\n",
    "The `objective()`-function contains the definition, training and evaluation of the classifier, which is done by a five-fold cross-validation split. <br>\n",
    "Last, the metrics are returned to enable a correct optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_values = ['sqrt','log2', None]\n",
    "\n",
    "space={\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 600, 10)),\n",
    "    'max_depth': hp.quniform('max_depth', 100, 400, 10),\n",
    "    'min_samples_split' : hp.uniform ('min_samples_split', 0, 0.5),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "    'max_features': hp.choice('max_features', max_features_values),\n",
    "}\n",
    "\n",
    "gm_scorer = make_scorer(geometric_mean_score, greater_is_better=True, average='macro') #Create Scorer for G-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    global X_train, y_train, X_test, y_test\n",
    "\n",
    "    # Create classifier\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators = int(space[\"n_estimators\"]),\n",
    "        max_depth = int(space[\"max_depth\"]),\n",
    "        min_samples_split = space[\"min_samples_split\"],\n",
    "        min_samples_leaf = space[\"min_samples_leaf\"],\n",
    "        max_features=space[\"max_features\"],\n",
    "        random_state=456,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Cross Validation\n",
    "    splits = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "    cross_val = cross_validate(rf_classifier, X_train, np.ravel(y_train), cv=splits, scoring={'f1_macro': 'f1_macro', 'f1_weighted': 'f1_weighted', 'auc': 'roc_auc_ovr', 'gmean': gm_scorer, 'precision': 'precision_macro', 'recall': 'recall_macro', 'waccuracy': 'balanced_accuracy'})\n",
    "    try:\n",
    "        cv_f1_macro = np.mean(cross_val.get('test_f1_macro')[~np.isnan(cross_val.get('test_f1_macro'))])\n",
    "        cv_f1_weighted = np.mean(cross_val.get('test_f1_weighted')[~np.isnan(cross_val.get('test_f1_weighted'))])\n",
    "        cv_auc = np.mean(cross_val.get('test_auc')[~np.isnan(cross_val.get('test_auc'))])\n",
    "        cv_gmean = np.mean(cross_val.get('test_gmean')[~np.isnan(cross_val.get('test_gmean'))])\n",
    "        cv_precision = np.mean(cross_val.get('test_precision')[~np.isnan(cross_val.get('test_precision'))])\n",
    "        cv_recall = np.mean(cross_val.get('test_recall')[~np.isnan(cross_val.get('test_recall'))])\n",
    "        cv_acc_weighted = np.mean(cross_val.get('test_waccuracy')[~np.isnan(cross_val.get('test_waccuracy'))])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\n",
    "            'loss': 1, \n",
    "            'status': STATUS_STRINGS[4], \n",
    "            'metrics': {\n",
    "                'cv_f1_macro': -1,\n",
    "                'cv_f1_weighted': -1,\n",
    "                'cv_auc': -1,\n",
    "                'cv_gmean': -1,\n",
    "                'cv_precision': -1,\n",
    "                'cv_recall': -1,\n",
    "                'cv_acc_weighted': -1\n",
    "            },\n",
    "            'eval_time': time.time()\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'loss': -cv_f1_macro, \n",
    "        'status': STATUS_OK, \n",
    "        'metrics': {\n",
    "            'cv_f1_macro': cv_f1_macro,\n",
    "            'cv_f1_weighted': cv_f1_weighted,\n",
    "            'cv_auc': cv_auc,\n",
    "            'cv_gmean': cv_gmean,\n",
    "            'cv_precision': cv_precision,\n",
    "            'cv_recall': cv_recall,\n",
    "            'cv_acc_weighted': cv_acc_weighted\n",
    "        },\n",
    "        'eval_time': time.time()\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimize Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials() #Parallelized optimization\n",
    "\n",
    "best_param = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=2,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators = int(best_param['n_estimators']),\n",
    "    max_depth = int(best_param[\"max_depth\"]),\n",
    "    min_samples_split = best_param[\"min_samples_split\"],\n",
    "    min_samples_leaf = best_param[\"min_samples_leaf\"],\n",
    "    max_features = max_features_values[best_param[\"max_features\"]],\n",
    "    random_state = 456,\n",
    "    n_jobs = -1,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "rf_classifier.fit(\n",
    "    X=X_train,\n",
    "    y=np.ravel(y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_classifier.predict(X_val) #Predict X_test\n",
    "pred_proba = rf_classifier.predict_proba(X_val) #Predict probablities X_test\n",
    "f1 = f1_score(y_val, pred, average=\"macro\") #Compute f1-score\n",
    "auc = roc_auc_score(np.ravel(y_val), pred_proba[:,1], average=\"macro\", multi_class=\"ovr\") #Compute AUC\n",
    "gmean = geometric_mean_score(y_val, pred, average=\"macro\") #Compute G-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1-Score:\", f1)\n",
    "print(\"AUC:\", auc)\n",
    "print(\"G-Mean:\", gmean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explain Classifier with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(rf_classifier)\n",
    "shap_values = explainer.shap_values(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
