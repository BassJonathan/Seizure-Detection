{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epileptic Seizure Classification with Bi-LSTM & Attention-Layer\n",
    "This notebook contains the classification of time series EEG data for the detection of epileptic seizures based on the preprocessed CHB-MIT Scalp EEG Database.\n",
    "The codes is structured as followed:\n",
    "1. [Imports](#1-imports)\n",
    "2. [Load Preprocessed Dataset](#2-load-preprocessed-dataset)\n",
    "3. [Split Dataset](#3-split-dataset)\n",
    "4. [Normalize Dataset](#4-normalize-dataset)\n",
    "5. [Bi-LSTM Model](#5-autoencoder) <br>\n",
    "5.1 [Define Neural Network-Model](#52-define-autoencoder-model) <br>\n",
    "5.2 [Compile Neural Network-Model]() <br>\n",
    "5.3 [Train Neural Network]() <br>\n",
    "6. [Validate Results](#6-validate-results)\n",
    "7. [Explain Model with SHAP]()\n",
    "8. [Conclusions](#8-conclusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import requiered libraries. <br>\n",
    "External packages can be installed via the `pip install` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Input, Attention\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "# from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Dataset\n",
    "In order to load the preprocessed dataset, that was created with the notebook `00_Preprocessing.ipynb`, is loaded and the numpy Arrays for the features and labels are extracted. <br>\n",
    "To enshure a functional distribution of the classes in the dataset, the classes with the respective amounts are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('../00_Data/Processed-Data/classification_dataset_max.npz')\n",
    "X = dataset[\"features\"]\n",
    "y = dataset[\"labels\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Dataset\n",
    "In order to validate and test the trained classifier, the dataset must be split into a `train`, `test`, and `validation` subset. <br>\n",
    "To preserve an equal distribution within each split, the `stratify`-option is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.4, shuffle=True, stratify=np.ravel(y), random_state=34)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5, shuffle=True, stratify=np.ravel(y_rest), random_state=34)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize Dataset\n",
    "When working with neural networks, it is imperative to normalize the data bevore training and testing. This enshures a faster training, avoids numerical instablities and provides a better generalization of the neural network. However with EEG-data, there are additional requirements due to the different characteristics and value-ranges of the individual channels. Therefore, the normalization is done channel by channel based on the training-subset and applied on the test- and validation-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X_train:np.ndarray, X_test:np.ndarray, X_val:np.ndarray, use_standard_scaler:bool=False) -> tuple:\n",
    "    if(use_standard_scaler):\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    X_train_norm = np.zeros(shape=(X_train.shape), dtype='float32')\n",
    "    X_test_norm = np.zeros(shape=(X_test.shape), dtype='float32')\n",
    "    X_val_norm = np.zeros(shape=(X_val.shape), dtype='float32')\n",
    "    for feature_col in range(X_train.shape[2]):\n",
    "        X_train_norm[:][:][feature_col] = scaler.fit_transform(X_train[:][:][feature_col])\n",
    "        X_test_norm[:][:][feature_col] = scaler.transform(X_test[:][:][feature_col])\n",
    "        X_val_norm[:][:][feature_col] = scaler.transform(X_val[:][:][feature_col])\n",
    "    return X_train_norm, X_test_norm, X_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm, X_test_norm, X_val_norm = normalize_features(X_train, X_test, X_val, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bi-LSTM Model\n",
    "This section contains the definition, compilation & training of the neural network.\n",
    "\n",
    "### 5.1 Define Neural Network-Model\n",
    "The neural network consists out of Bi-LSTM layers followed by an attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(train_shape:tuple, initial_lr:float=0.0001):\n",
    "    inputs = Input(shape=(train_shape[1], train_shape[2]))\n",
    "    bilstm1 = Bidirectional(LSTM(64, return_sequences = True, kernel_regularizer=L1L2(0, 0.001)))(inputs)\n",
    "    do1 = Dropout(0.5)(bilstm1)\n",
    "    bilstm2 = Bidirectional(LSTM(32, return_sequences = True, kernel_regularizer=L1L2(0, 0.001)))(do1)\n",
    "    at = Attention(32)([bilstm2, bilstm2])\n",
    "    d1 = Dense(16, kernel_regularizer=L1L2(0, 0.001))(at)\n",
    "    do3 = Dropout(0.5)(d1)\n",
    "    d2 = Dense(8, kernel_regularizer=L1L2(0, 0.0001))(do3)\n",
    "    outputs = Dense(1, activation='sigmoid')(d2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=initial_lr)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compile Neural Network-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_model(X_train.shape, 0.0001)\n",
    "\n",
    "earlystopper = EarlyStopping(patience=25, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0000001, verbose=1, cooldown=10)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Fit Neural Network-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=50, \n",
    "    batch_size=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1, \n",
    "    callbacks=[earlystopper, reduce_lr] #, TqdmCallback(verbose=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data = [\n",
    "        go.Scatter(y=history.history['loss'], name=\"train\"),\n",
    "        go.Scatter(y=history.history['val_loss'], name=\"val\"),\n",
    "    ],\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [MSE]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Model Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = model.predict(X_test)\n",
    "y_test_predictions = (y_test_predictions >= 0.5).astype(int)\n",
    "f1score = f1_score(y_test, y_test_predictions)\n",
    "gm = geometric_mean_score(y_test, y_test_predictions, average=\"binary\")\n",
    "auc = roc_auc_score(y_test, y_test_predictions, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_test_predictions)\n",
    "recall = recall_score(y_test, y_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(len(y_test_predictions)):\n",
    "    a.append(y_test_predictions[i][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions2 = (np.array(a) >= 0.5).astype(int)\n",
    "f1score = f1_score(y_test, y_test_predictions2)\n",
    "gm = geometric_mean_score(y_test, y_test_predictions2, average=\"binary\")\n",
    "auc = roc_auc_score(y_test, y_test_predictions2, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_test_predictions2)\n",
    "recall = recall_score(y_test, y_test_predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1score, gm, auc, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max      0.8529284164859002 0.8558157521673803 0.8558649977293616 0.8592657342657343 0.8466838931955211\n",
    "# Majority 0.7218001168907071 0.7578027296919185 0.774371772170577 0.8734087694483734 0.6150398406374502\n",
    "# Max+Atte 0.9416648459689753 0.9429646733706349 0.9430840468336327 0.9556541019955654 0.9280792420327304"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
