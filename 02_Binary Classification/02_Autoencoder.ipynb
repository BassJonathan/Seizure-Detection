{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epileptic Seizure Classification with Random Forest\n",
    "This notebook contains the classification of time series EEG data for the detection of epileptic seizures based on the preprocessed CHB-MIT Scalp EEG Database.\n",
    "The codes is structured as followed:\n",
    "1. [Imports](#1-imports)\n",
    "2. [Load Preprocessed Dataset](#2-load-preprocessed-dataset)\n",
    "3. [Split Dataset](#3-split-dataset)\n",
    "4. [Normalize Dataset](#4-normalize-dataset)\n",
    "5. [Autoencoder](#5-autoencoder) <br>\n",
    "5.1 [Seperate Normal & Anomaly Data](#51-seperate-normal--anomaly-data) <br>\n",
    "5.2 [Define Autoencoder-Model](#52-define-autoencoder-model) <br>\n",
    "5.3 [Compile Autoencoder-Model](#53-compile-autoencoder-model) <br>\n",
    "5.4 [Train Autoencoder](#54-train-autoencoder-model) <br>\n",
    "6. [Visualize Reconstruction Error](#6-visualize-reconstruction-error)\n",
    "7. [Binary Classification](#7-binary-classification)\n",
    "8. [Conclusions](#8-conclusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import requiered libraries. <br>\n",
    "External packages can be installed via the `pip install` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Input, RepeatVector, TimeDistributed, Flatten, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Dataset\n",
    "In order to load the preprocessed dataset, that was created with the notebook `00_Preprocessing.ipynb`, is loaded and the numpy Arrays for the features and labels are extracted. <br>\n",
    "To enshure a functional distribution of the classes in the dataset, the classes with the respective amounts are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('../00_Data/Processed-Data/classification_dataset_max.npz')\n",
    "X = dataset[\"features\"]\n",
    "y = dataset[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes: \", X.shape, y.shape)\n",
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Dataset\n",
    "In order to validate and test the trained classifier, the dataset must be split into a `train`, `test`, and `validation` subset. <br>\n",
    "To preserve an equal distribution within each split, the `stratify`-option is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.4, shuffle=True, stratify=np.ravel(y), random_state=34)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5, shuffle=True, stratify=np.ravel(y_rest), random_state=34)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize Dataset\n",
    "When working with neural networks, it is imperative to normalize the data bevore training and testing. This enshures a faster training, avoids numerical instablities and provides a better generalization of the neural network. However with EEG-data, there are additional requirements due to the different characteristics and value-ranges of the individual channels. Therefore, the normalization is done channel by channel based on the training-subset and applied on the test- and validation-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X_train:np.ndarray, X_test:np.ndarray, X_val:np.ndarray, use_standard_scaler:bool=False) -> tuple:\n",
    "    if(use_standard_scaler):\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    X_train_norm = np.zeros(shape=(X_train.shape), dtype='float32')\n",
    "    X_test_norm = np.zeros(shape=(X_test.shape), dtype='float32')\n",
    "    X_val_norm = np.zeros(shape=(X_val.shape), dtype='float32')\n",
    "    for feature_col in range(X_train.shape[2]):\n",
    "        X_train_norm[:][:][feature_col] = scaler.fit_transform(X_train[:][:][feature_col])\n",
    "        X_test_norm[:][:][feature_col] = scaler.transform(X_test[:][:][feature_col])\n",
    "        X_val_norm[:][:][feature_col] = scaler.transform(X_val[:][:][feature_col])\n",
    "    return X_train_norm, X_test_norm, X_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized, X_test_normalized, X_val_normalized = normalize_features(X_train, X_test, X_val, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autoencoder\n",
    "The following section contains the data-preperation, build and training of the autoencoder. <br>\n",
    "\n",
    "<b>What is an autoencoder?</b><br>\n",
    "An autoencoder is a neural network architecture, that is used for unsupervised machine learning tasks. It consists out of two main components: The encoder & decoder. The encoder takes the input data and transforms it into a lower dimensional representation of the data, the so-called \"latent-space\". The decoder-part takes this data and tries to reconstruct the original input data. The main target during the training-phase is to minimize the reconstruction error. <br>\n",
    "\n",
    "<b>How can autoencoders be used for the detection of epileptic seizures in EEG-data?</b><br>\n",
    "There are two options how autoencoders can be used for the detection of epileptic seizures in EEG-data: Reconstruction-Error & Latent-Space. <br>\n",
    "By training the autoencoder only on data that does not contain any epileptic seizures, the reconstruction error for \"normal\" data is minimized. That means that if a sample with an active seizure is predicted, the reconstruction error will be increased. By defining an error-threshold, a binary classification can be performed to seperate normal samples from samples with an epileptic seizure.\n",
    "\n",
    "The second option is to use the latent space for the classification. The autoencoder is trained on the complete dataset with the same task of minimizing the reconstruction error. By seperating the decoding-component from the autoencoder, the latent space is exposed. Because of the differences in the data when an epileptic seizure is present, the representation of these samples must be different in the reduced space. Based on this assumption, a classification by using a clustering-approach can be done.\n",
    "\n",
    "The following code contains the first approach.\n",
    "\n",
    "### 5.1 Seperate Normal & Anomaly Data\n",
    "In order to detect anomalies in the EEG-data that indicate an epileptic seizure, the autoencoder is only trained on samples where no seizure is present. Therefore, the samples must be seperated into \"normal\" and \"anomalie\" data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normal = X_train_normalized[np.where(y_train == 0)[0]]\n",
    "X_train_anomalies = X_train_normalized[np.where(y_train == 1)[0]]\n",
    "\n",
    "X_val_normal = X_val_normalized[np.where(y_val == 0)[0]]\n",
    "X_val_anomalies = X_val_normalized[np.where(y_val == 1)[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Define Autoencoder-Model\n",
    "As described, an autoencoder consists out of a encoder and decoder component. The encoder reduces the dimensionality of the data and the decoder tries to reconstruct the input data. The resulting neural network model can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_model(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = LSTM(50, return_sequences=True)(inputs)\n",
    "    do1 = Dropout(0.5)(L1)\n",
    "    L2 = LSTM(40, return_sequences=True)(do1)\n",
    "    do2 = Dropout(0.5)(L2)\n",
    "    L3 = LSTM(20, return_sequences=True)(do2)\n",
    "    do3 = Dropout(0.5)(L3)\n",
    "    L4 = LSTM(15, return_sequences=False)(do3)\n",
    "    L5 = RepeatVector(X.shape[1])(L4)\n",
    "    L6 = LSTM(15, return_sequences=True)(L5)\n",
    "    do4 = Dropout(0.5)(L6)\n",
    "    L7 = LSTM(20, return_sequences=True)(do4)\n",
    "    do5 = Dropout(0.5)(L7)\n",
    "    L8 = LSTM(40, return_sequences=True)(do5)\n",
    "    do6 = Dropout(0.5)(L8)\n",
    "    L9 = LSTM(50, return_sequences=True)(do6)\n",
    "    output = TimeDistributed(Dense(X.shape[2]))(L9)   \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Compile Autoencoder-Model\n",
    "After defining the neural network, the model must be compiled. In addition the optimizer and loss-function must be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = autoencoder_model(X_train_normal)\n",
    "\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.00001)\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(patience=25, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=0.0000001, verbose=1, cooldown=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Train Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(\n",
    "    X_train_normal, \n",
    "    X_train_normal,\n",
    "    epochs=100,\n",
    "    batch_size=50,\n",
    "    validation_data=(X_val_normal, X_val_normal),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopper, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data = [\n",
    "        go.Scatter(y=history.history['loss'], name=\"train\"),\n",
    "        go.Scatter(y=history.history['val_loss'], name=\"val\"),\n",
    "    ],\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [MSE]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Model Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_pred_normal = autoencoder.predict(X_val_normal)\n",
    "mse = np.mean(np.square(X_val_normal - X_val_pred_normal), axis=(1, 2))\n",
    "fig = px.histogram(x=mse, nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_pred_anomalies = autoencoder.predict(X_val_anomalies)\n",
    "mse = np.mean(np.square(X_val_anomalies - X_val_pred_anomalies), axis=(1, 2))\n",
    "fig = px.histogram(x=mse, nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder.save('./AE_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.transpose(X_train_normal, (0,2,1))\n",
    "data = []\n",
    "for i in a[0]:\n",
    "    data.append(\n",
    "        go.Scatter(y=i)\n",
    "    )\n",
    "\n",
    "fig = go.Figure(\n",
    "    data = data,\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [MSE]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Model Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = autoencoder.predict(X_train_normal[:1])\n",
    "a = np.transpose(pred, (0,2,1))\n",
    "data = []\n",
    "for i in a[0]:\n",
    "    data.append(\n",
    "        go.Scatter(y=i)\n",
    "    )\n",
    "\n",
    "fig = go.Figure(\n",
    "    data = data,\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [MSE]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Model Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pred = autoencoder.predict(X_test_normalized)\n",
    "mse = np.mean(np.square(X_test_pred - X_test_normalized), axis=(1, 2))\n",
    "y_test_predictions = np.where(mse > 0.1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score = f1_score(y_test, y_test_predictions)\n",
    "gm = geometric_mean_score(y_test, y_test_predictions, average=\"binary\")\n",
    "auc = roc_auc_score(y_test, y_test_predictions, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_test_predictions)\n",
    "recall = recall_score(y_test, y_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1score, gm, auc, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st try: 0.6601055484239052 0.06778158069014832 0.500582460274657 0.49349541480059717 0.9965546942291128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
