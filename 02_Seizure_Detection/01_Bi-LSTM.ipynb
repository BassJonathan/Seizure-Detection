{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('../00_Data/Processed-Data/classification_dataset.npz')\n",
    "X = dataset[\"features\"]\n",
    "y = dataset[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.4, shuffle=True, stratify=np.ravel(y), random_state=34)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5, shuffle=True, stratify=np.ravel(y_rest), random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X_train:np.ndarray, X_test:np.ndarray, X_val:np.ndarray, use_standard_scaler:bool=False) -> tuple:\n",
    "    if(use_standard_scaler):\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    X_train_norm = np.zeros(shape=(X_train.shape), dtype='float32')\n",
    "    X_test_norm = np.zeros(shape=(X_test.shape), dtype='float32')\n",
    "    X_val_norm = np.zeros(shape=(X_val.shape), dtype='float32')\n",
    "    for feature_col in range(X_train.shape[2]):\n",
    "        X_train_norm[:][:][feature_col] = scaler.fit_transform(X_train[:][:][feature_col])\n",
    "        X_test_norm[:][:][feature_col] = scaler.transform(X_test[:][:][feature_col])\n",
    "        X_val_norm[:][:][feature_col] = scaler.transform(X_val[:][:][feature_col])\n",
    "    return X_train_norm, X_test_norm, X_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm, X_test_norm, X_val_norm = normalize_features(X_train, X_test, X_val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(train_shape:tuple, initial_lr:float=0.000001):\n",
    "    inputs = Input(shape=(train_shape[1], train_shape[2]))\n",
    "    bilstm1 = LSTM(64, return_sequences = True)(inputs)\n",
    "    do5 = Dropout(0.5)(bilstm1)\n",
    "    bilstm2 = LSTM(32)(do5)\n",
    "    do6 = Dropout(0.5)(bilstm2)\n",
    "    d1 = Dense(16)(do6)\n",
    "    do7 = Dropout(0.5)(d1)\n",
    "    d2 = Dense(8)(do7)\n",
    "    outputs = Dense(1, activation='sigmoid')(d2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=initial_lr)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_model(X_train_norm.shape, 0.000001)\n",
    "\n",
    "earlystopper = EarlyStopping(patience=25, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0000001, verbose=1, cooldown=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1, \n",
    "    callbacks=[earlystopper, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data = [\n",
    "        go.Scatter(y=history.history['loss'], name=\"train\"),\n",
    "        go.Scatter(y=history.history['val_loss'], name=\"val\"),\n",
    "    ],\n",
    "    layout = {\"yaxis\": {\"title\": \"Loss [MSE]\"}, \"xaxis\": {\"title\": \"Epoch\"}, \"title\": \"Model Loss over Epochs\"}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
